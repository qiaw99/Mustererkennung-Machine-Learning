{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ãœbung 8 Neuronal Network - Rainier Robles & Valentin Wolf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-22T20:45:28.244888Z",
     "start_time": "2017-12-22T20:45:28.239596Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-23T13:18:11.953033Z",
     "start_time": "2017-12-23T13:18:11.944572Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_y(y,total_classes=10):\n",
    "    \"\"\"gets 1-D vector with the classes (all positive and consecutive)\n",
    "    and converts it to an matrix of shape (classes,Ys) \n",
    "    with a 1 at where the class is and 0s everywhere else\"\"\"\n",
    "    y=y.astype('int8')\n",
    "    if total_classes == None:\n",
    "        total_classes = np.unique(y)\n",
    "        \n",
    "    new_y = np.zeros((y.shape[0],total_classes))\n",
    "    for i in range(y.shape[0]):\n",
    "        new_y[i,y[i]] = 1\n",
    "    return new_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-23T13:18:48.936272Z",
     "start_time": "2017-12-23T13:18:48.430385Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_table('zip.train', delim_whitespace=True,header=None)\n",
    "test = pd.read_table('zip.test', delim_whitespace=True,header=None)\n",
    "\n",
    "#split labels y from data X\n",
    "y_train = transform_y(train[0].as_matrix(),total_classes=10)\n",
    "y_tt = train[0].as_matrix()\n",
    "X_train = train.drop(0, axis=1).as_matrix()\n",
    "\n",
    "y_test = transform_y(test[0].as_matrix(),total_classes=10)\n",
    "X_test = test.drop(0, axis=1).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-23T13:24:15.507890Z",
     "start_time": "2017-12-23T13:24:15.501867Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier():\n",
    "    def error_rate(self,truth, pred):\n",
    "        \"\"\"gets two vectors, returns (wrongly classified / total)\"\"\"\n",
    "        return 1 - self.accuracy(truth, pred)\n",
    "    \n",
    "    def accuracy(self,truth,pred):\n",
    "        return 1 - (np.abs((truth-pred)).sum(axis=1)/2).mean()# np.mean(truth == pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-23T13:24:54.884695Z",
     "start_time": "2017-12-23T13:24:54.554309Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuronalNetwork(Classifier):\n",
    "    def __init__(self,sizes):\n",
    "        \n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        self.acc_rates = []\n",
    "    \n",
    "    def train(self, X, y, epochs=10, learning_rate=0.5, printfreq=25, X_test=None,y_test=None):\n",
    "        \"\"\"\n",
    "        X: datapoints\n",
    "        y: lables as rows in matrix with 1 when class and 0 else \n",
    "        \"\"\"\n",
    "        learning_rate = learning_rate/X.shape[0]\n",
    "        for i in range(epochs):\n",
    "            if i % printfreq == 0 or i == epochs-1:\n",
    "                try:\n",
    "                    print(\"Iteration {} | Training Accuracy: {} | Test Accuracy: {}\".format(\n",
    "                                i,self.predict_acc(X,y),self.predict_acc(X_test,y_test)))\n",
    "                    self.acc_rates.append(self.predict_acc(X_test,y_test))\n",
    "                except AttributeError: \n",
    "                    print(\"Iteration {} | Training Accuracy: {}\".format(i,self.predict_acc(X,y)))\n",
    "\n",
    "            for j in range(X.shape[0]):\n",
    "                deltas_b, deltas_w = self.backprop(X[j][np.newaxis],y[j])\n",
    "                self.weights = [w-(learning_rate*dw)\n",
    "                            for w, dw in zip(self.weights, deltas_w)] \n",
    "                self.biases = [(b-(learning_rate*db) )\n",
    "                            for b, db in zip(self.biases, deltas_b)]\n",
    "    \n",
    "    def backprop(self,x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x.T\n",
    "        activations = [x.T] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta     #np.sum(delta.T,axis=0)[np.newaxis].T\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            #np.sum(delta.T,axis=0)[np.newaxis].T\n",
    "            #print(nabla_b[-l].shape)\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        output_activations -= y[np.newaxis].T\n",
    "\n",
    "        return (output_activations) \n",
    "    \n",
    "    def sigmoid(self,z):\n",
    "        return 1.0/(1.0+np.exp(-z))\n",
    "    \n",
    "    def sigmoid_deriv(self,z):\n",
    "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
    "        \n",
    "    def feedforward(self,X):\n",
    "        a = X.T\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = self.sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "    \n",
    "    def predict(self,X):\n",
    "        col_y = np.argmax(self.feedforward(X),axis=0)\n",
    "        return transform_y(col_y,total_classes=10)\n",
    "    \n",
    "    def predict_error(self,X,y):\n",
    "        return self.error_rate(self.predict(X),y)\n",
    "    def predict_acc(self,X,y):\n",
    "        return 1 - self.predict_error(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-22T20:58:02.736057Z",
     "start_time": "2017-12-22T20:58:02.731202Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-22T20:36:31.917763Z",
     "start_time": "2017-12-22T20:36:31.912596Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-23T13:27:32.690352Z",
     "start_time": "2017-12-23T13:26:25.974941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 | Training Accuracy: 0.22767795912769162 | Test Accuracy: 0.22919780767314402\n",
      "Iteration 1 | Training Accuracy: 0.6069126320120697 | Test Accuracy: 0.5635276532137519\n",
      "Iteration 2 | Training Accuracy: 0.749691400356604 | Test Accuracy: 0.6970602889885401\n",
      "Iteration 3 | Training Accuracy: 0.7880949115347688 | Test Accuracy: 0.7354260089686099\n",
      "Iteration 4 | Training Accuracy: 0.8107255520504733 | Test Accuracy: 0.7538614848031888\n",
      "Iteration 5 | Training Accuracy: 0.8221094500068578 | Test Accuracy: 0.7668161434977578\n",
      "Iteration 6 | Training Accuracy: 0.8308873954190097 | Test Accuracy: 0.7737917289486796\n",
      "Iteration 7 | Training Accuracy: 0.8376080098751886 | Test Accuracy: 0.7802690582959642\n",
      "Iteration 8 | Training Accuracy: 0.8429570703607188 | Test Accuracy: 0.7867463876432487\n",
      "Iteration 9 | Training Accuracy: 0.8480318200521191 | Test Accuracy: 0.7887394120577977\n",
      "Iteration 10 | Training Accuracy: 0.8509120833904814 | Test Accuracy: 0.7897359242650722\n",
      "Iteration 11 | Training Accuracy: 0.8529694143464546 | Test Accuracy: 0.789237668161435\n",
      "Iteration 12 | Training Accuracy: 0.8562611438760115 | Test Accuracy: 0.7932237169905332\n",
      "Iteration 13 | Training Accuracy: 0.8621588259498011 | Test Accuracy: 0.796711509715994\n",
      "Iteration 14 | Training Accuracy: 0.9209984912906323 | Test Accuracy: 0.8545092177379173\n",
      "Iteration 15 | Training Accuracy: 0.9336167878206008 | Test Accuracy: 0.8659691081215745\n",
      "Iteration 16 | Training Accuracy: 0.937731449732547 | Test Accuracy: 0.872446437468859\n",
      "Iteration 17 | Training Accuracy: 0.9404745576738445 | Test Accuracy: 0.8729446935724963\n",
      "Iteration 18 | Training Accuracy: 0.9437662872034015 | Test Accuracy: 0.8749377179870453\n",
      "Iteration 19 | Training Accuracy: 0.9473323275270882 | Test Accuracy: 0.874439461883408\n",
      "Iteration 20 | Training Accuracy: 0.9487038814977369 | Test Accuracy: 0.8794220229197808\n",
      "Iteration 21 | Training Accuracy: 0.9495268138801262 | Test Accuracy: 0.8819133034379671\n",
      "Iteration 22 | Training Accuracy: 0.9518584556302291 | Test Accuracy: 0.8849028400597907\n",
      "Iteration 23 | Training Accuracy: 0.9532300096008778 | Test Accuracy: 0.8858993522670653\n",
      "Iteration 24 | Training Accuracy: 0.9558359621451105 | Test Accuracy: 0.8868958644743398\n",
      "Iteration 25 | Training Accuracy: 0.9569332053216294 | Test Accuracy: 0.8878923766816144\n",
      "Iteration 26 | Training Accuracy: 0.9585790700864079 | Test Accuracy: 0.8893871449925261\n",
      "Iteration 27 | Training Accuracy: 0.9591276916746674 | Test Accuracy: 0.8878923766816144\n",
      "Iteration 28 | Training Accuracy: 0.9599506240570567 | Test Accuracy: 0.8868958644743398\n",
      "Iteration 29 | Training Accuracy: 0.9613221780277054 | Test Accuracy: 0.8878923766816144\n",
      "Iteration 30 | Training Accuracy: 0.9626937319983542 | Test Accuracy: 0.8883906327852517\n",
      "Iteration 31 | Training Accuracy: 0.9629680427924839 | Test Accuracy: 0.8873941205779771\n",
      "Iteration 32 | Training Accuracy: 0.9637909751748731 | Test Accuracy: 0.8883906327852517\n",
      "Iteration 33 | Training Accuracy: 0.963928130571938 | Test Accuracy: 0.8893871449925261\n",
      "Iteration 34 | Training Accuracy: 0.9642024413660677 | Test Accuracy: 0.890881913303438\n",
      "Iteration 35 | Training Accuracy: 0.9648882183513922 | Test Accuracy: 0.8918784255107125\n",
      "Iteration 36 | Training Accuracy: 0.9655739953367165 | Test Accuracy: 0.8918784255107125\n",
      "Iteration 37 | Training Accuracy: 0.9657111507337813 | Test Accuracy: 0.8928749377179871\n",
      "Iteration 38 | Training Accuracy: 0.9663969277191058 | Test Accuracy: 0.8943697060288989\n",
      "Iteration 39 | Training Accuracy: 0.9668083939103004 | Test Accuracy: 0.8958644743398106\n",
      "Iteration 40 | Training Accuracy: 0.9676313262926897 | Test Accuracy: 0.8973592426507224\n",
      "Iteration 41 | Training Accuracy: 0.9681799478809491 | Test Accuracy: 0.8973592426507224\n",
      "Iteration 42 | Training Accuracy: 0.9688657248662735 | Test Accuracy: 0.898355754857997\n",
      "Iteration 43 | Training Accuracy: 0.9690028802633384 | Test Accuracy: 0.898355754857997\n",
      "Iteration 44 | Training Accuracy: 0.9696886572486627 | Test Accuracy: 0.8993522670652716\n",
      "Iteration 45 | Training Accuracy: 0.9702372788369222 | Test Accuracy: 0.8993522670652716\n",
      "Iteration 46 | Training Accuracy: 0.9706487450281168 | Test Accuracy: 0.8998505231689088\n",
      "Iteration 47 | Training Accuracy: 0.9717459882046359 | Test Accuracy: 0.8998505231689088\n",
      "Iteration 48 | Training Accuracy: 0.9717459882046359 | Test Accuracy: 0.9003487792725461\n",
      "Iteration 49 | Training Accuracy: 0.9720202989987656 | Test Accuracy: 0.9013452914798207\n",
      "Iteration 50 | Training Accuracy: 0.9724317651899602 | Test Accuracy: 0.9013452914798207\n",
      "Iteration 51 | Training Accuracy: 0.9725689205870252 | Test Accuracy: 0.9028400597907325\n",
      "Iteration 52 | Training Accuracy: 0.9729803867782197 | Test Accuracy: 0.9023418036870952\n",
      "Iteration 53 | Training Accuracy: 0.9732546975723495 | Test Accuracy: 0.9028400597907325\n",
      "Iteration 54 | Training Accuracy: 0.973803319160609 | Test Accuracy: 0.9038365719980069\n",
      "Iteration 55 | Training Accuracy: 0.973803319160609 | Test Accuracy: 0.9048330842052815\n",
      "Iteration 56 | Training Accuracy: 0.9739404745576739 | Test Accuracy: 0.9053313403089188\n",
      "Iteration 57 | Training Accuracy: 0.9740776299547387 | Test Accuracy: 0.905829596412556\n",
      "Iteration 58 | Training Accuracy: 0.9742147853518036 | Test Accuracy: 0.9068261086198306\n",
      "Iteration 59 | Training Accuracy: 0.9747634069400631 | Test Accuracy: 0.9068261086198306\n",
      "Iteration 60 | Training Accuracy: 0.9753120285283225 | Test Accuracy: 0.9053313403089188\n",
      "Iteration 61 | Training Accuracy: 0.9758606501165821 | Test Accuracy: 0.9053313403089188\n",
      "Iteration 62 | Training Accuracy: 0.9761349609107118 | Test Accuracy: 0.905829596412556\n",
      "Iteration 63 | Training Accuracy: 0.9764092717048416 | Test Accuracy: 0.9063278525161933\n",
      "Iteration 64 | Training Accuracy: 0.9766835824989714 | Test Accuracy: 0.9073243647234679\n",
      "Iteration 65 | Training Accuracy: 0.9768207378960362 | Test Accuracy: 0.9063278525161933\n",
      "Iteration 66 | Training Accuracy: 0.9769578932931011 | Test Accuracy: 0.9063278525161933\n",
      "Iteration 67 | Training Accuracy: 0.977095048690166 | Test Accuracy: 0.905829596412556\n",
      "Iteration 68 | Training Accuracy: 0.9772322040872309 | Test Accuracy: 0.9053313403089188\n",
      "Iteration 69 | Training Accuracy: 0.9776436702784255 | Test Accuracy: 0.9053313403089188\n",
      "Iteration 70 | Training Accuracy: 0.9779179810725552 | Test Accuracy: 0.9053313403089188\n",
      "Iteration 71 | Training Accuracy: 0.9779179810725552 | Test Accuracy: 0.9043348281016442\n",
      "Iteration 72 | Training Accuracy: 0.9780551364696201 | Test Accuracy: 0.9038365719980069\n",
      "Iteration 73 | Training Accuracy: 0.9783294472637498 | Test Accuracy: 0.9033383158943697\n",
      "Iteration 74 | Training Accuracy: 0.9783294472637498 | Test Accuracy: 0.9038365719980069\n",
      "Iteration 75 | Training Accuracy: 0.9783294472637498 | Test Accuracy: 0.9038365719980069\n",
      "Iteration 76 | Training Accuracy: 0.9783294472637498 | Test Accuracy: 0.9043348281016442\n",
      "Iteration 77 | Training Accuracy: 0.9786037580578796 | Test Accuracy: 0.9048330842052815\n",
      "Iteration 78 | Training Accuracy: 0.9788780688520093 | Test Accuracy: 0.9048330842052815\n",
      "Iteration 79 | Training Accuracy: 0.9788780688520093 | Test Accuracy: 0.9048330842052815\n",
      "Iteration 80 | Training Accuracy: 0.9788780688520093 | Test Accuracy: 0.9038365719980069\n",
      "Iteration 81 | Training Accuracy: 0.9788780688520093 | Test Accuracy: 0.9038365719980069\n",
      "Iteration 82 | Training Accuracy: 0.9794266904402689 | Test Accuracy: 0.9038365719980069\n",
      "Iteration 83 | Training Accuracy: 0.9794266904402689 | Test Accuracy: 0.9033383158943697\n",
      "Iteration 84 | Training Accuracy: 0.9795638458373337 | Test Accuracy: 0.9033383158943697\n",
      "Iteration 85 | Training Accuracy: 0.9797010012343985 | Test Accuracy: 0.9033383158943697\n",
      "Iteration 86 | Training Accuracy: 0.9797010012343985 | Test Accuracy: 0.9033383158943697\n",
      "Iteration 87 | Training Accuracy: 0.9799753120285283 | Test Accuracy: 0.9028400597907325\n",
      "Iteration 88 | Training Accuracy: 0.9799753120285283 | Test Accuracy: 0.9028400597907325\n",
      "Iteration 89 | Training Accuracy: 0.9799753120285283 | Test Accuracy: 0.9028400597907325\n",
      "Iteration 90 | Training Accuracy: 0.9799753120285283 | Test Accuracy: 0.9033383158943697\n",
      "Iteration 91 | Training Accuracy: 0.980386778219723 | Test Accuracy: 0.9033383158943697\n",
      "Iteration 92 | Training Accuracy: 0.9805239336167878 | Test Accuracy: 0.9033383158943697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 93 | Training Accuracy: 0.9806610890138527 | Test Accuracy: 0.9038365719980069\n",
      "Iteration 94 | Training Accuracy: 0.9806610890138527 | Test Accuracy: 0.9038365719980069\n",
      "Iteration 95 | Training Accuracy: 0.9806610890138527 | Test Accuracy: 0.9043348281016442\n",
      "Iteration 96 | Training Accuracy: 0.9806610890138527 | Test Accuracy: 0.9048330842052815\n",
      "Iteration 97 | Training Accuracy: 0.9809353998079824 | Test Accuracy: 0.9048330842052815\n",
      "Iteration 98 | Training Accuracy: 0.9807982444109176 | Test Accuracy: 0.9048330842052815\n",
      "Iteration 99 | Training Accuracy: 0.9807982444109176 | Test Accuracy: 0.9053313403089188\n"
     ]
    }
   ],
   "source": [
    "NN = NeuronalNetwork([256,30,10])\n",
    "NN.train(X_train,y_train,epochs=100,learning_rate=300,X_test=X_test,y_test=y_test,printfreq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-23T12:56:54.194616Z",
     "start_time": "2017-12-23T12:56:54.191450Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test=None\n",
    "if test:\n",
    "    print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-18T10:06:38.293405Z",
     "start_time": "2017-12-18T10:06:38.282172Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:70: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  2.71312537e-09,   1.88792083e-18,   2.56342138e-23, ...,\n",
       "          7.20961547e-09,   1.00000000e+00,   6.66001531e-29],\n",
       "       [  1.53059479e-28,   1.75232593e-20,   6.38859710e-32, ...,\n",
       "          2.12023081e-23,   1.03657748e-41,   4.82482829e-05],\n",
       "       [  3.65769523e-12,   7.37669141e-07,   2.14328249e-27, ...,\n",
       "          2.91136723e-18,   1.17539040e-17,   8.24125668e-11],\n",
       "       ..., \n",
       "       [  3.51050106e-44,   4.16771004e-21,   7.38725341e-31, ...,\n",
       "          1.17311509e-20,   6.85843159e-36,   8.57527208e-23],\n",
       "       [  5.12691575e-29,   5.62544447e-23,   7.67575998e-25, ...,\n",
       "          6.07369836e-01,   2.32138719e-23,   2.30585616e-02],\n",
       "       [  5.51519053e-23,   5.77968633e-15,   1.19604969e-13, ...,\n",
       "          2.13177786e-05,   9.63112125e-13,   4.51834623e-14]])"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.feedforward(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-17T14:02:30.699082Z",
     "start_time": "2017-12-17T14:02:30.695167Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 7291)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.biases[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-23T13:23:34.420002Z",
     "start_time": "2017-12-23T13:23:34.408021Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80839391030037033"
      ]
     },
     "execution_count": 724,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.abs((y_train-NN.predict(X_train))).sum(axis=1)/2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-23T12:52:32.962156Z",
     "start_time": "2017-12-23T12:52:32.954711Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:82: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9, 4, 3, ..., 4, 0, 1])"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = NN.predict(X_test)\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#confusion_matrix(pred,y_test)\n",
    "pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.argmax(pred,axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sizes = [4,3,4]\n",
    "list(zip(sizes[:-1], sizes[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(range(5,0,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.concatenate((np.random.randn(2, 3),np.ones((2,1))),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-17T13:10:13.802314Z",
     "start_time": "2017-12-17T13:10:13.798322Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-17T13:17:11.551947Z",
     "start_time": "2017-12-17T13:17:11.547560Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1.00991919, -0.36287491, -0.52331782,  1.68611696, -0.55968326,\n",
       "         0.31156315,  1.15918928, -0.64807042, -0.19311074, -0.45522268])]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-17T13:27:44.042143Z",
     "start_time": "2017-12-17T13:27:43.993824Z"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-221-cdcd56c925eb>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-221-cdcd56c925eb>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    def _backprop(self,X,y):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    def _backprop(self,X,y):\n",
    "        \n",
    "        #feedforward\n",
    "        activation = X\n",
    "        activations = [X]\n",
    "        local_derivs = []\n",
    "        \n",
    "        w = self.weights\n",
    "        b = self.biases\n",
    "        for i in range(self.num_layers-1):\n",
    "            activation = np.dot(w[i],activation.T).T + b[i]\n",
    "            local_derivs.append(activation)\n",
    "            activation = self.sigmoid(activation)\n",
    "            activations.append(activation) #braucht man das?\n",
    "        \n",
    "        #calculate loss\n",
    "        #print(activations[-1])\n",
    "        #print(\"Train Err: \", self.error_rate(np.argmax(activations[-1],axis=1),y))\n",
    "        #print(np.argmax(activations[-1],axis=1))\n",
    "        \n",
    "        cost_deriv = y - activations[-1]\n",
    "        #print(cost_deriv.shape)\n",
    "        #for i in range(cost_deriv.shape[0]):\n",
    "        #    cost_deriv[i,int(y[i])] -= 1\n",
    "        \n",
    "        \n",
    "        #backwards\n",
    "        deltas_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        deltas_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        delta = cost_deriv * self.sigmoid_deriv(local_derivs[-1])\n",
    "        #print(delta)\n",
    "        deltas_b[-1] = np.sum(delta,axis=0)\n",
    "        deltas_w[-1] = np.dot(delta.T, activations[-2])\n",
    "        \n",
    "        for i in range(2,self.num_layers):\n",
    "            local_deriv = self.sigmoid_deriv(local_derivs[-i])\n",
    "            delta = np.dot(self.weights[-i+1].T, delta.T) * local_deriv.T\n",
    "            print(delta)\n",
    "            deltas_b[-i] = np.sum(delta.T,axis=0)\n",
    "            deltas_w[-i] = np.dot(delta, activations[-i-1])\n",
    "            \n",
    "        #print(deltas_b)\n",
    "        #print(deltas_w)\n",
    "        return (deltas_b, deltas_w)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-17T13:42:01.016981Z",
     "start_time": "2017-12-17T13:42:00.643302Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "\n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)\n",
    "\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test));\n",
    "            else:\n",
    "                print(\"Epoch {} complete\".format(j))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-17T13:49:59.426646Z",
     "start_time": "2017-12-17T13:49:59.423644Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Network([256,30,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-17T13:50:01.529390Z",
     "start_time": "2017-12-17T13:50:01.510979Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = X_train.T\n",
    "y = y_train\n",
    "b,w = net.backprop(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-17T13:51:44.701227Z",
     "start_time": "2017-12-17T13:51:44.697402Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 256)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-17T13:52:14.802809Z",
     "start_time": "2017-12-17T13:52:14.798827Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 256)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-17T13:49:11.966396Z",
     "start_time": "2017-12-17T13:49:11.890159Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuronalNetwork(Classifier):\n",
    "    def __init__(self,sizes):\n",
    "        #self.sizes = sizes\n",
    "        #self.layers = []\n",
    "        #self.weights = []\n",
    "        #self.biases = []\n",
    "        \n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    \n",
    "    def train(self, X, y, iterations=10, learning_rate=0.5):\n",
    "        lr = learning_rate/X.shape[0]\n",
    "        for i in range(iterations):\n",
    "            #print(X[i%X.shape[0]][np.newaxis].shape)\n",
    "            for j in range(1):#X.shape[0]):\n",
    "                deltas_b, deltas_w = self.backprop(X,y)#[j][np.newaxis],y[j])\n",
    "                self.weights = [w-(learning_rate*dw)\n",
    "                            for w, dw in zip(self.weights, deltas_w)] \n",
    "                self.biases = [(b-(learning_rate*db) )\n",
    "                            for b, db in zip(self.biases, deltas_b)]\n",
    "    \n",
    "    def backprop(self,x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x.T\n",
    "        activations = [x.T] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = np.sum(delta.T,axis=0)[np.newaxis].T\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = np.sum(delta.T,axis=0)[np.newaxis].T\n",
    "            #print(nabla_b[-l].shape)\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "\n",
    "        output_activations[y.astype(int)] -= 1\n",
    "\n",
    "        return (output_activations) \n",
    "    \n",
    "    def sigmoid(self,z):\n",
    "        return 1.0/(1.0+np.exp(-z))\n",
    "    \n",
    "    def sigmoid_deriv(self,z):\n",
    "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
    "        \n",
    "    def feedforward(self,X):\n",
    "        a = X.T\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = self.sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.feedforward(X),axis=0)\n",
    "    \n",
    "    def predict_error(self,X,y):\n",
    "        return self.error_rate(self.predict(X),y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-17T13:49:19.333581Z",
     "start_time": "2017-12-17T13:49:19.314346Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MyNetwork' object has no attribute 'cost_derivative'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-281-404f52b5f630>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-279-fe5fcf917fb8>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m             \u001b[0msigmoid_prime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mnabla_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mnabla_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MyNetwork' object has no attribute 'cost_derivative'"
     ]
    }
   ],
   "source": [
    "net = MyNetwork([256,30,10])\n",
    "net.backprop(X_train.T,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-18T10:03:23.220054Z",
     "start_time": "2017-12-18T10:03:23.215893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.,  5.,  4., ...,  3.,  0.,  1.])"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
