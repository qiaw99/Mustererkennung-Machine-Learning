{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.671</td>\n",
       "      <td>4</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.450</td>\n",
       "      <td>11</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.76</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.022</td>\n",
       "      <td>9.744</td>\n",
       "      <td>445</td>\n",
       "      <td>1257</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.729</td>\n",
       "      <td>43</td>\n",
       "      <td>749</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9  ...    48     49  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00 ...  0.00  0.000   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94 ...  0.00  0.132   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25 ...  0.01  0.143   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00  0.137   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63 ...  0.00  0.135   \n",
       "5  0.00  0.00  0.00  0.0  1.85  0.00  0.00  1.85  0.00  0.00 ...  0.00  0.223   \n",
       "6  0.00  0.00  0.00  0.0  1.92  0.00  0.00  0.00  0.00  0.64 ...  0.00  0.054   \n",
       "7  0.00  0.00  0.00  0.0  1.88  0.00  0.00  1.88  0.00  0.00 ...  0.00  0.206   \n",
       "8  0.15  0.00  0.46  0.0  0.61  0.00  0.30  0.00  0.92  0.76 ...  0.00  0.271   \n",
       "9  0.06  0.12  0.77  0.0  0.19  0.32  0.38  0.00  0.06  0.00 ...  0.04  0.030   \n",
       "\n",
       "    50     51     52     53     54   55    56  57  \n",
       "0  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "5  0.0  0.000  0.000  0.000  3.000   15    54   1  \n",
       "6  0.0  0.164  0.054  0.000  1.671    4   112   1  \n",
       "7  0.0  0.000  0.000  0.000  2.450   11    49   1  \n",
       "8  0.0  0.181  0.203  0.022  9.744  445  1257   1  \n",
       "9  0.0  0.244  0.081  0.000  1.729   43   749   1  \n",
       "\n",
       "[10 rows x 58 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('spambase.data',delim_whitespace=False,header=None)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Switching non-spam labels from 0 to -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in range(df.shape[0]):\n",
    "    if df.at[row,57] == 0:\n",
    "        df.at[row,57] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalisation of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = df.as_matrix()\n",
    "features = all_data[:,:-1]\n",
    "labels = all_data[:,-1]\n",
    "mu = np.mean(features,axis=0)\n",
    "var = features.var(axis=0)\n",
    "normalised = (features-mu)/np.sqrt(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# don't forget to add a column of ones!\n",
    "constant_coefficient = np.ones((labels.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.concatenate((constant_coefficient,normalised,labels.reshape((labels.shape[0],1))),axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "data = shuffle(data,random_state=1338)\n",
    "\n",
    "N = data.shape[0]\n",
    "split = 0.2\n",
    "y_train = data[int(split*N):, -1]\n",
    "X_train = data[int(split*N):, :-1]\n",
    "\n",
    "y_test = data[:int(split*N), -1]\n",
    "X_test = data[:int(split*N), :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual Logistic Regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def calc_prob(self,y,Beta,x):\n",
    "        return 1 / (1 + exp(-y * np.dot(Beta.T, x)))\n",
    "    \n",
    "    def train(self,X_train,y_train,iterations=5):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.Beta = np.mean(self.X_train,axis=0)\n",
    "        delta = 0.01\n",
    "        P = np.full(self.X_train.shape[0],0.5)\n",
    "        for iteration in range(iterations):\n",
    "            for row in range(self.X_train.shape[0]):\n",
    "                P[row] = self.calc_prob(self.y_train[row],self.Beta,self.X_train[row,:])\n",
    "            sum_corrections = np.sum((self.y_train.reshape((self.y_train.shape[0],1))*self.X_train*(1-P.reshape((P.shape[0],1)))),axis=0)\n",
    "            self.Beta = self.Beta + (delta*sum_corrections)\n",
    "            #delta = delta / 10\n",
    "            \n",
    "    def predict(self,X_test,y_test):\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.test_rows = y_test.shape[0]\n",
    "        predictions = []\n",
    "        for row in range(X_test.shape[0]):\n",
    "            pos_prob = self.calc_prob(1,self.Beta,X_test[row,:])\n",
    "            neg_prob = 1 - pos_prob\n",
    "            if pos_prob >= neg_prob:\n",
    "                predictions.append(1)\n",
    "            else:\n",
    "                predictions.append(-1)\n",
    "        self.pred_array = np.array(predictions)\n",
    "        \n",
    "    def error_rate(self):\n",
    "        bools = np.equal(self.pred_array,self.y_test.flatten())\n",
    "        correct = np.sum(bools)\n",
    "        return (self.test_rows-correct)/self.test_rows\n",
    "        #return pred\n",
    "        \n",
    "    def changelabels(self,arrtochange):\n",
    "        \"\"\"in labels, changes all positive numbers to 0s and all negative numbers to 1s\"\"\"\n",
    "        result = np.zeros((arrtochange.size))\n",
    "        for i in range(arrtochange.size):\n",
    "            if arrtochange[i] >= 0:\n",
    "                result[i] = 0\n",
    "            else:\n",
    "                result[i] = 1\n",
    "        return result\n",
    "    \n",
    "    def confusion_matrix(self):\n",
    "        preds = self.changelabels(self.pred_array).flatten()\n",
    "        testlabels = self.y_test\n",
    "        changedtestlabels = self.changelabels(testlabels)\n",
    "        mtrx = np.zeros((2,2))\n",
    "        for i in range(self.test_rows):\n",
    "            mtrx[int(changedtestlabels[i]),int(preds[i])] += 1\n",
    "        return mtrx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing up to 20 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate for 1 iterations = 0.113043478261\n",
      "Success rate for 1 iterations = 0.886956521739\n",
      "[[ 314.   68.]\n",
      " [  36.  502.]]\n",
      "Error rate for 2 iterations = 0.101086956522\n",
      "Success rate for 2 iterations = 0.898913043478\n",
      "[[ 321.   61.]\n",
      " [  32.  506.]]\n",
      "Error rate for 3 iterations = 0.0989130434783\n",
      "Success rate for 3 iterations = 0.901086956522\n",
      "[[ 322.   60.]\n",
      " [  31.  507.]]\n",
      "Error rate for 4 iterations = 0.095652173913\n",
      "Success rate for 4 iterations = 0.904347826087\n",
      "[[ 326.   56.]\n",
      " [  32.  506.]]\n",
      "Error rate for 5 iterations = 0.0880434782609\n",
      "Success rate for 5 iterations = 0.911956521739\n",
      "[[ 333.   49.]\n",
      " [  32.  506.]]\n",
      "Error rate for 6 iterations = 0.0923913043478\n",
      "Success rate for 6 iterations = 0.907608695652\n",
      "[[ 334.   48.]\n",
      " [  37.  501.]]\n",
      "Error rate for 7 iterations = 0.0869565217391\n",
      "Success rate for 7 iterations = 0.913043478261\n",
      "[[ 340.   42.]\n",
      " [  38.  500.]]\n",
      "Error rate for 8 iterations = 0.0902173913043\n",
      "Success rate for 8 iterations = 0.909782608696\n",
      "[[ 334.   48.]\n",
      " [  35.  503.]]\n",
      "Error rate for 9 iterations = 0.0858695652174\n",
      "Success rate for 9 iterations = 0.914130434783\n",
      "[[ 342.   40.]\n",
      " [  39.  499.]]\n",
      "Error rate for 10 iterations = 0.0880434782609\n",
      "Success rate for 10 iterations = 0.911956521739\n",
      "[[ 333.   49.]\n",
      " [  32.  506.]]\n",
      "Error rate for 11 iterations = 0.0891304347826\n",
      "Success rate for 11 iterations = 0.910869565217\n",
      "[[ 339.   43.]\n",
      " [  39.  499.]]\n",
      "Error rate for 12 iterations = 0.0902173913043\n",
      "Success rate for 12 iterations = 0.909782608696\n",
      "[[ 330.   52.]\n",
      " [  31.  507.]]\n",
      "Error rate for 13 iterations = 0.11847826087\n",
      "Success rate for 13 iterations = 0.88152173913\n",
      "[[ 334.   48.]\n",
      " [  61.  477.]]\n",
      "Error rate for 14 iterations = 0.0923913043478\n",
      "Success rate for 14 iterations = 0.907608695652\n",
      "[[ 331.   51.]\n",
      " [  34.  504.]]\n",
      "Error rate for 15 iterations = 0.1\n",
      "Success rate for 15 iterations = 0.9\n",
      "[[ 326.   56.]\n",
      " [  36.  502.]]\n",
      "Error rate for 16 iterations = 0.102173913043\n",
      "Success rate for 16 iterations = 0.897826086957\n",
      "[[ 335.   47.]\n",
      " [  47.  491.]]\n",
      "Error rate for 17 iterations = 0.122826086957\n",
      "Success rate for 17 iterations = 0.877173913043\n",
      "[[ 302.   80.]\n",
      " [  33.  505.]]\n",
      "Error rate for 18 iterations = 0.125\n",
      "Success rate for 18 iterations = 0.875\n",
      "[[ 347.   35.]\n",
      " [  80.  458.]]\n",
      "Error rate for 19 iterations = 0.122826086957\n",
      "Success rate for 19 iterations = 0.877173913043\n",
      "[[ 291.   91.]\n",
      " [  22.  516.]]\n",
      "Error rate for 20 iterations = 0.139130434783\n",
      "Success rate for 20 iterations = 0.860869565217\n",
      "[[ 357.   25.]\n",
      " [ 103.  435.]]\n",
      "Lowest error rate at 9 iterations with error rate of 0.0858695652174\n"
     ]
    }
   ],
   "source": [
    "iterations = list(range(1,21))\n",
    "errors = []\n",
    "for iteration in iterations:\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.train(X_train,y_train,iterations=iteration)\n",
    "    logreg.predict(X_test,y_test)\n",
    "    errors.append(logreg.error_rate())\n",
    "    print(\"Error rate for\",iteration,\"iterations =\",logreg.error_rate())\n",
    "    print(\"Success rate for\",iteration,\"iterations =\",1-logreg.error_rate())\n",
    "    print(logreg.confusion_matrix())\n",
    "    \n",
    "print(\"Lowest error rate at\",iterations[np.argmin(np.array(errors))],\"iterations with error rate of\",errors[np.argmin(np.array(errors))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x150a2d3748>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x150a2d3780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.xticks(iterations)\n",
    "plt.plot(iterations, [100*e for e in errors], 'bo-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification for and, or, xor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# again not forgetting to add the constant coefficient at the start\n",
    "# here we replace 0 with -1 for the labels\n",
    "and_ = np.array([[1,0,0,-1],[1,0,1,-1],[1,1,0,-1],[1,1,1,1]])\n",
    "or_ = np.array([[1,0,0,-1],[1,0,1,1],[1,1,0,1],[1,1,1,1]])\n",
    "xor_ = np.array([[1,0,0,-1],[1,0,1,1],[1,1,0,1],[1,1,1,-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate for and with 1 iteration = 0.75\n",
      "[[ 1.  0.]\n",
      " [ 3.  0.]]\n",
      "Error rate for and with 100 iterations = 0.0\n",
      "[[ 1.  0.]\n",
      " [ 0.  3.]]\n"
     ]
    }
   ],
   "source": [
    "and_test = LogisticRegression()\n",
    "and_test.train(and_[:,:-1],and_[:,-1],iterations=1)\n",
    "and_test.predict(and_[:,:-1],and_[:,-1])\n",
    "print(\"Error rate for and with 1 iteration =\",and_test.error_rate())\n",
    "print(and_test.confusion_matrix())\n",
    "\n",
    "and_test = LogisticRegression()\n",
    "and_test.train(and_[:,:-1],and_[:,-1],iterations=100)\n",
    "and_test.predict(and_[:,:-1],and_[:,-1])\n",
    "print(\"Error rate for and with 100 iterations =\",and_test.error_rate())\n",
    "print(and_test.confusion_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate for or with 1 iteration = 0.25\n",
      "[[ 3.  0.]\n",
      " [ 1.  0.]]\n",
      "Error rate for or with 500 iterations = 0.0\n",
      "[[ 3.  0.]\n",
      " [ 0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "or_test = LogisticRegression()\n",
    "or_test.train(or_[:,:-1],or_[:,-1],iterations=1)\n",
    "or_test.predict(or_[:,:-1],or_[:,-1])\n",
    "print(\"Error rate for or with 1 iteration =\",or_test.error_rate())\n",
    "print(or_test.confusion_matrix())\n",
    "\n",
    "or_test = LogisticRegression()\n",
    "or_test.train(or_[:,:-1],or_[:,-1],iterations=500)\n",
    "or_test.predict(or_[:,:-1],or_[:,-1])\n",
    "print(\"Error rate for or with 500 iterations =\",or_test.error_rate())\n",
    "print(or_test.confusion_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate for xor with 1 iteration = 0.5\n",
      "[[ 2.  0.]\n",
      " [ 2.  0.]]\n",
      "Error rate for xor with 300 iterations = 0.25\n",
      "[[ 2.  0.]\n",
      " [ 1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "xor_test = LogisticRegression()\n",
    "xor_test.train(xor_[:,:-1],xor_[:,-1],iterations=1)\n",
    "xor_test.predict(xor_[:,:-1],xor_[:,-1])\n",
    "print(\"Error rate for xor with 1 iteration =\",xor_test.error_rate())\n",
    "print(xor_test.confusion_matrix())\n",
    "\n",
    "# tested up to more than 1000 iterations and we don't get a better result than a 25% error rate\n",
    "xor_test = LogisticRegression()\n",
    "xor_test.train(xor_[:,:-1],xor_[:,-1],iterations=300)\n",
    "xor_test.predict(xor_[:,:-1],xor_[:,-1])\n",
    "print(\"Error rate for xor with 300 iterations =\",xor_test.error_rate())\n",
    "print(xor_test.confusion_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
